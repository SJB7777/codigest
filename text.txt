'''D:\02_Projects\Dev\X-ray_AI\Reflecto\main.py'''
import torch
from torch.amp import GradScaler
from torch.optim import AdamW
from torch.utils.data import DataLoader

from reflecto.dataset import DatasetH5, ParamQuantizer
from reflecto.model import XRRClassifier, train_epoch, validate_epoch


def main():
    # ---------------------------------------------------
    # 기본 설정
    # ---------------------------------------------------
    h5_path = r"D:\03_Resources\Data\XRR_AI\data\p300o6_raw.h5"
    model_path = r"xrr_model2.pt"
    batch_size = 32
    epochs = 20
    lr = 1e-3

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # ---------------------------------------------------
    # 1. Quantizer 생성
    # ---------------------------------------------------
    quantizer = ParamQuantizer(
        thickness_bins=None,
        roughness_bins=None,
        sld_bins=None
    )

    n_th = len(quantizer.thickness_bins) - 1
    n_rg = len(quantizer.roughness_bins) - 1
    n_sld = len(quantizer.sld_bins) - 1

    # ---------------------------------------------------
    # 2. Dataset / DataLoader 구성
    # ---------------------------------------------------
    train_dataset = DatasetH5(h5_path, quantizer)
    val_dataset   = DatasetH5(h5_path, quantizer)

    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=0,
        pin_memory=True,
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=0,
        pin_memory=True,
    )

    # ---------------------------------------------------
    # 3. reflectivity 길이 자동 감지
    # ---------------------------------------------------
    sample_refl, sample_label = train_dataset[0]
    q_len = sample_refl.shape[0]
    n_layers = sample_label.shape[0]
    print(f"[INFO] q_len={q_len}, layers={n_layers}")

    # ---------------------------------------------------
    # 4. 모델 생성 + compile
    # ---------------------------------------------------
    model = XRRClassifier(
        q_len=q_len,
        n_layers=n_layers,
        n_th_bins=n_th,
        n_rg_bins=n_rg,
        n_sld_bins=n_sld
    ).to(device)

    optimizer = AdamW(model.parameters(), lr=lr, weight_decay=1e-5)
    scaler = GradScaler("cuda")  # AMP 안정성

    # ---------------------------------------------------
    # 5. 학습 루프
    # ---------------------------------------------------
    for epoch in range(1, epochs + 1):

        train_loss = train_epoch(
            model, train_loader, optimizer, device, scaler
        )

        val_loss, val_accs = validate_epoch(
            model, val_loader, device
        )

        print(
            f"[Epoch {epoch:02d}] "
            f"Train Loss: {train_loss:.4f} | "
            f"Val Loss: {val_loss:.4f} | "
            f"Acc(th/rg/sld): "
            f"{val_accs['th']:.3f} / {val_accs['rg']:.3f} / {val_accs['sld']:.3f}"
        )

    # ---------------------------------------------------
    # 6. 모델 + quantizer 저장
    # ---------------------------------------------------
    ckpt = {
        "model": model.state_dict(),
        "quantizer": quantizer.state_dict()
    }
    torch.save(ckpt, model_path)
    print(f"[INFO] Saved: {model_path}")


if __name__ == "__main__":
    main()

'''D:\02_Projects\Dev\X-ray_AI\Reflecto\src\reflecto\dataset.py'''
"""
Improved Dataset + Quantizer for XRR project.

Changes:
- ParamQuantizer: added state_dict / load_state_dict / fit_from_dataset
- DatasetH5: safe per-worker HDF5 opening, optional log-scaling, normalization, downsampling.
- __getitem__ returns CPU tensors (no device pinned in dataset).
"""

from __future__ import annotations

import h5py
import numpy as np
import torch
from torch.utils.data import Dataset

from reflecto.simulator.simulator import ParamSet


class ParamQuantizer:
    """
    Binning (quantization) utility for thickness / roughness / sld.

    Usage:
      q = ParamQuantizer()
      q.fit_from_dataset(path, n_sample=1000)  # optional, build bins from data
      q.state_dict() / q.load_state_dict(...) for saving/loading
      q.quantize(param) -> np.ndarray([t_idx, r_idx, s_idx])
    """

    def __init__(
        self,
        thickness_bins: np.ndarray | None = None,
        roughness_bins: np.ndarray | None = None,
        sld_bins: np.ndarray | None = None
    ):
        # sensible defaults (units: nm for thickness, Å for roughness, 1e-6 Å^-2 or user convention)
        self.thickness_bins = np.linspace(0.0, 200.0, 51) if thickness_bins is None else np.asarray(thickness_bins)
        self.roughness_bins = np.linspace(0.0, 10.0, 21) if roughness_bins is None else np.asarray(roughness_bins)
        self.sld_bins = np.linspace(1.0, 6.0, 26) if sld_bins is None else np.asarray(sld_bins)

    # --------------------------
    # Persistence
    # --------------------------
    def state_dict(self):
        return {
            "thickness_bins": self.thickness_bins.astype(float).tolist(),
            "roughness_bins": self.roughness_bins.astype(float).tolist(),
            "sld_bins": self.sld_bins.astype(float).tolist(),
        }

    def load_state_dict(self, state: dict):
        self.thickness_bins = np.asarray(state["thickness_bins"], dtype=float)
        self.roughness_bins = np.asarray(state["roughness_bins"], dtype=float)
        self.sld_bins = np.asarray(state["sld_bins"], dtype=float)

    # --------------------------
    # Fitting helper
    # --------------------------
    def fit_from_dataset(self, h5_path: str, sample_n: int = 1000, rng_seed: int = 0):
        """Estimate reasonable bins from real dataset values (sample subset)."""
        rng = np.random.default_rng(rng_seed)
        with h5py.File(h5_path, "r") as hf:
            n = hf["R"].shape[0]
            idx = rng.choice(n, size=min(sample_n, n), replace=False)
            thickness = hf["thickness"][idx]  # shape (k, L)
            roughness = hf["roughness"][idx]
            sld = hf["sld"][idx]

        # flatten and compute percentiles
        tvals = np.asarray(thickness).ravel()
        rvals = np.asarray(roughness).ravel()
        svals = np.asarray(sld).ravel()

        # avoid NaNs
        tvals = tvals[np.isfinite(tvals)]
        rvals = rvals[np.isfinite(rvals)]
        svals = svals[np.isfinite(svals)]

        # set bins using logspace for thickness if range large
        tmin, tmax = max(tvals.min(), 1e-6), tvals.max()
        if tmax / max(tmin, 1e-12) > 50:
            # use logspace for thickness
            self.thickness_bins = np.logspace(np.log10(tmin), np.log10(tmax), 51)
        else:
            self.thickness_bins = np.linspace(tmin, tmax, 51)

        self.roughness_bins = np.linspace(max(0.0, rvals.min()), rvals.max(), 21)
        self.sld_bins = np.linspace(svals.min(), svals.max(), 26)

    # --------------------------
    # Quantize API
    # --------------------------
    @staticmethod
    def _quantize_array(vals: np.ndarray, bins: np.ndarray) -> np.ndarray:
        """Vectorized quantization. Returns indices clipped to valid range [0, len(bins)-2]."""
        idx = np.digitize(vals, bins) - 1
        idx = np.clip(idx, 0, len(bins) - 2)
        return idx.astype(np.int64)

    def quantize(self, param: ParamSet) -> np.ndarray:
        """Accept ParamSet (single) or array-like -> returns np.ndarray labels."""
        t = float(param.thickness)
        r = float(param.roughness)
        s = float(param.sld)
        return np.array([
            self._quantize_array(np.asarray([t]), self.thickness_bins)[0],
            self._quantize_array(np.asarray([r]), self.roughness_bins)[0],
            self._quantize_array(np.asarray([s]), self.sld_bins)[0],
        ], dtype=np.int64)

    def quantize_multi(self, thickness_arr: np.ndarray, roughness_arr: np.ndarray, sld_arr: np.ndarray) -> np.ndarray:
        """Vectorized quantize for arrays per-layer. Returns shape (n_layers, 3)."""
        t_idx = self._quantize_array(np.asarray(thickness_arr).ravel(), self.thickness_bins)
        r_idx = self._quantize_array(np.asarray(roughness_arr).ravel(), self.roughness_bins)
        s_idx = self._quantize_array(np.asarray(sld_arr).ravel(), self.sld_bins)
        return np.stack([t_idx, r_idx, s_idx], axis=1)


class DatasetH5(Dataset):
    """
    HDF5 dataset for XRR reflectivity and layer parameters.

    Key features:
      - per-process lazy HDF5 file opening (safe with num_workers>0)
      - optional log scaling and normalization (mean/std computed or provided)
      - returns (refl_tensor, labels_tensor) with CPU tensors (float32 / int64)
    """

    def __init__(
        self,
        h5_path: str,
        quantizer: ParamQuantizer,
        log_scale: bool = True,
        normalize: bool = True,
        downsample: int | None = None,
        stats: dict | None = None,
        prefetch_sample_n: int = 1000,
    ):
        self.h5_path = h5_path
        self.quantizer = quantizer
        self.log_scale = bool(log_scale)
        self.normalize = bool(normalize)
        self.downsample = None if downsample is None else int(downsample)
        self._hf = None  # per-process handle lazy opened
        self._length = None

        # open briefly to get length
        with h5py.File(self.h5_path, "r") as hf:
            self._length = hf["R"].shape[0]

        # compute stats if requested (mean/std on log(R))
        if stats is not None:
            self.mean = float(stats.get("mean", 0.0))
            self.std = float(stats.get("std", 1.0))
        elif self.normalize:
            # sample subset to estimate normalization
            self.mean, self.std = self._estimate_stats(prefetch_sample_n)
        else:
            self.mean, self.std = 0.0, 1.0

    def __len__(self) -> int:
        return int(self._length)

    # --------------------------
    # HDF5 access helpers
    # --------------------------
    def _ensure_open(self) -> h5py.File:
        """Open file per-process (safe with num_workers spawn/fork)."""
        if self._hf is None:
            self._hf = h5py.File(self.h5_path, "r")

    def _estimate_stats(self, n_sample: int = 1000) -> tuple[float, float]:
        """Estimate mean/std of log10(R) using random subset."""
        rng = np.random.default_rng(0)
        with h5py.File(self.h5_path, "r") as hf:
            n = hf["R"].shape[0]
            idx = rng.choice(n, size=min(n_sample, n), replace=False)
            vals = []
            for i in idx:
                R = hf["R"][i]
                if self.log_scale:
                    # guard against zeros/negatives
                    R = np.asarray(R, dtype=float)
                    R = np.where(R <= 0, 1e-15, R)
                    vals.append(np.log10(R))
                else:
                    vals.append(np.asarray(R, dtype=float))
            arr = np.concatenate([v.ravel() for v in vals], axis=0)
        arr = arr[np.isfinite(arr)]
        if arr.size == 0:
            return 0.0, 1.0
        return float(arr.mean()), float(arr.std() if arr.std() > 0 else 1.0)

    # --------------------------
    # Data conversion
    # --------------------------
    def _process_reflectivity(self, refl: np.ndarray) -> np.ndarray:
        """Apply log-scaling, downsampling, normalization and return float32 array."""
        arr = np.asarray(refl, dtype=float).ravel()
        if self.downsample is not None and self.downsample > 0:
            # simple uniform downsampling
            if arr.size > self.downsample:
                idx = np.linspace(0, arr.size - 1, num=self.downsample, dtype=int)
                arr = arr[idx]
        if self.log_scale:
            arr = np.where(arr <= 0, 1e-15, arr)
            arr = np.log10(arr)
        if self.normalize:
            arr = (arr - self.mean) / (self.std if self.std != 0 else 1.0)
        return arr.astype(np.float32)

    def __getitem__(self, idx):
        self._ensure_open()
        hf = self._hf

        refl = hf["R"][idx]           # (q_len,) or (q_len, ...)
        thick = hf["thickness"][idx]  # (n_layers,)
        rough = hf["roughness"][idx]
        sld = hf["sld"][idx]

        # process reflectivity -> cpu numpy
        refl_arr = self._process_reflectivity(refl)

        # quantize labels vectorized
        labels = self.quantizer.quantize_multi(np.asarray(thick), np.asarray(rough), np.asarray(sld))
        # labels shape (n_layers, 3) as int64

        # convert to torch tensors (CPU). Device transfer is left to DataLoader / training loop.
        refl_t = torch.from_numpy(refl_arr).to(dtype=torch.float32)
        label_t = torch.from_numpy(np.asarray(labels, dtype=np.int64))

        return refl_t, label_t

    # optional convenience
    def get_normalization(self) -> dict:
        return {"mean": float(self.mean), "std": float(self.std)}

'''D:\02_Projects\Dev\X-ray_AI\Reflecto\src\reflecto\evaluate.py'''
import warnings
from pathlib import Path

import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from tqdm import tqdm

from reflecto.dataset import DatasetH5, ParamQuantizer
from reflecto.model import XRRClassifier


class XRREvaluator:
    def __init__(
        self,
        h5_path: Path | str,
        checkpoint_path: Path | str,
        batch_size: int = 64,
        device: str | torch.device = None,
        num_workers: int = 0,   # Windows에서는 0~2 권장
        pin_memory: bool = True,
    ):
        self.h5_path = Path(h5_path)
        self.checkpoint_path = Path(checkpoint_path)
        self.device = device or torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.batch_size = batch_size
        self.num_workers = num_workers
        self.pin_memory = pin_memory

        # Quantizer
        self.quantizer = ParamQuantizer(thickness_bins=None, roughness_bins=None, sld_bins=None)
        self.n_th = len(self.quantizer.thickness_bins) - 1
        self.n_rg = len(self.quantizer.roughness_bins) - 1
        self.n_sld = len(self.quantizer.sld_bins) - 1

        # Dataset
        self.val_dataset = DatasetH5(self.h5_path, self.quantizer)
        self.val_loader = DataLoader(
            self.val_dataset,
            batch_size=self.batch_size,
            shuffle=False,
            num_workers=self.num_workers,
            pin_memory=self.pin_memory
        )

        # Model
        sample_refl, sample_label = self.val_dataset[0]
        self.q_len = sample_refl.shape[0]
        self.n_layers = sample_label.shape[0]

        self.model = XRRClassifier(
            q_len=self.q_len,
            n_layers=self.n_layers,
            n_th_bins=self.n_th,
            n_rg_bins=self.n_rg,
            n_sld_bins=self.n_sld
        ).to(self.device)
        # --- Safe checkpoint loading ---
        if self.checkpoint_path and self.checkpoint_path.is_file():
            try:
                state = torch.load(self.checkpoint_path, map_location=self.device)
                self.model.load_state_dict(state)
                print(f"Checkpoint '{self.checkpoint_path}' successfully loaded.")
            except Exception as e:
                warnings.warn(f"Faild to load checkpoint: {e}\n→ Evaluation will proceed with a newly initialized model.", stacklevel=2)
        else:
            warnings.warn("No checkpoint file found. Evaluation will proceed with a randomly initialized model.", stacklevel=2)
        # self.model.load_state_dict(torch.load(self.checkpoint_path, map_location=self.device))
        self.model.eval()

        # Loss
        self.ce = nn.CrossEntropyLoss()

    def evaluate(self, use_amp: bool = False, logits_clip: float = 50.0):
        """모든 평가 데이터를 계산하고 결과 저장"""
        all_tgt_th, all_tgt_rg, all_tgt_sld = [], [], []
        all_pred_th, all_pred_rg, all_pred_sld = [], [], []
        all_losses = []

        self.model.eval()

        pbar = tqdm(self.val_loader, desc="Evaluating")
        with torch.no_grad():
            for refl, labels in pbar:
                refl = refl.to(self.device)
                labels = labels.to(self.device)

                # --- AMP 실제 적용 ---
                if use_amp and self.device.type == "cuda":
                    with torch.amp.autocast("cuda"):
                        th_logits, rg_logits, sld_logits = self.model(refl)
                else:
                    th_logits, rg_logits, sld_logits = self.model(refl)

                # --- logits clipping (학습과 동일하게) ---
                if logits_clip is not None:
                    th_logits = th_logits.clamp(-logits_clip, logits_clip)
                    rg_logits = rg_logits.clamp(-logits_clip, logits_clip)
                    sld_logits = sld_logits.clamp(-logits_clip, logits_clip)

                # --- target ---
                tgt_th = labels[:, :, 0]
                tgt_rg = labels[:, :, 1]
                tgt_sld = labels[:, :, 2]

                # --- Loss 계산(CrossEntropy 조합) ---
                loss = (
                    self.ce(th_logits.reshape(-1, th_logits.shape[-1]),
                            tgt_th.reshape(-1)) +
                    self.ce(rg_logits.reshape(-1, rg_logits.shape[-1]),
                            tgt_rg.reshape(-1)) +
                    self.ce(sld_logits.reshape(-1, sld_logits.shape[-1]),
                            tgt_sld.reshape(-1))
                )
                all_losses.append(loss.item())

                # --- 예측 저장 ---
                all_tgt_th.append(tgt_th.cpu().numpy())
                all_tgt_rg.append(tgt_rg.cpu().numpy())
                all_tgt_sld.append(tgt_sld.cpu().numpy())

                all_pred_th.append(th_logits.argmax(dim=-1).cpu().numpy())
                all_pred_rg.append(rg_logits.argmax(dim=-1).cpu().numpy())
                all_pred_sld.append(sld_logits.argmax(dim=-1).cpu().numpy())

                pbar.set_postfix({"loss": f"{loss.item():.4f}"})

        # --- Flatten ---
        self.all_tgt_th = np.concatenate(all_tgt_th).ravel()
        self.all_tgt_rg = np.concatenate(all_tgt_rg).ravel()
        self.all_tgt_sld = np.concatenate(all_tgt_sld).ravel()

        self.all_pred_th = np.concatenate(all_pred_th).ravel()
        self.all_pred_rg = np.concatenate(all_pred_rg).ravel()
        self.all_pred_sld = np.concatenate(all_pred_sld).ravel()

        self.all_losses = all_losses

        # --- Accuracy ---
        self.acc_th = (self.all_pred_th == self.all_tgt_th).mean()
        self.acc_rg = (self.all_pred_rg == self.all_tgt_rg).mean()
        self.acc_sld = (self.all_pred_sld == self.all_tgt_sld).mean()
        self.avg_loss = np.mean(all_losses)

        return {
            "loss": self.avg_loss,
            "acc_th": self.acc_th,
            "acc_rg": self.acc_rg,
            "acc_sld": self.acc_sld
        }

    def plot_loss(self, window: int = 50):
        losses = np.array(self.all_losses)
        plt.figure(figsize=(8, 4))

        # Raw loss
        plt.plot(losses, alpha=0.4, label="Raw Loss")

        # Moving average
        if len(losses) >= window:
            ma = np.convolve(losses, np.ones(window)/window, mode="valid")
            plt.plot(range(window-1, window-1+len(ma)), ma, linewidth=2, label=f"MA({window})")

        plt.xlabel("Batch")
        plt.ylabel("Loss")
        plt.title("Validation Loss (Raw + Moving Average)")
        plt.legend()
        plt.grid(alpha=0.3)
        plt.show()

    def plot_error_histogram(self):
        plt.figure(figsize=(15, 4))

        for i, (tgt, pred, name) in enumerate(zip(
            [self.all_tgt_th, self.all_tgt_rg, self.all_tgt_sld],
            [self.all_pred_th, self.all_pred_rg, self.all_pred_sld],
            ["Thickness", "Roughness", "SLD"], strict=False
        )):
            plt.subplot(1, 3, i+1)
            errors = pred - tgt

            sns.histplot(errors, bins=40, kde=True)
            mean = errors.mean()
            std = errors.std()

            plt.axvline(mean, color="r", linestyle="--", label=f"Mean = {mean:.2f}")
            plt.axvline(mean+std, color="g", linestyle=":", label=f"+1σ = {mean+std:.2f}")
            plt.axvline(mean-std, color="g", linestyle=":", label=f"-1σ = {mean-std:.2f}")

            plt.title(f"{name} Error Distribution")
            plt.xlabel("Prediction - Target")
            plt.legend()

        # plt.tight_layout()
        plt.show()

    def plot_scatter(self, use_hexbin: bool = False):
        plt.figure(figsize=(15, 4))

        for i, (tgt, pred, name) in enumerate(zip(
            [self.all_tgt_th, self.all_tgt_rg, self.all_tgt_sld],
            [self.all_pred_th, self.all_pred_rg, self.all_pred_sld],
            ["Thickness", "Roughness", "SLD"], strict=False
        )):
            plt.subplot(1, 3, i+1)

            if use_hexbin:
                plt.hexbin(tgt, pred, gridsize=40, cmap="viridis", mincnt=1)
                plt.colorbar(label="Density")
            else:
                plt.scatter(tgt, pred, alpha=0.2, s=8)

            # y = x선 (이상적 예측)
            tmin, tmax = tgt.min(), tgt.max()
            plt.plot([tmin, tmax], [tmin, tmax], "r--", linewidth=1.5, label="Ideal")

            # 추세선 (회귀)
            coef = np.polyfit(tgt, pred, 1)
            poly = np.poly1d(coef)
            plt.plot([tmin, tmax], poly([tmin, tmax]), "b-", linewidth=1.2, label=f"Fit: y={coef[0]:.2f}x+{coef[1]:.2f}")

            plt.title(f"{name}: Target vs Predicted")
            plt.xlabel("Target")
            plt.ylabel("Predicted")
            plt.legend()

        # plt.tight_layout()
        plt.show()

    def plot_correlation(self):
        import pandas as pd

        df_target = pd.DataFrame({
            "Thickness": self.all_tgt_th,
            "Roughness": self.all_tgt_rg,
            "SLD": self.all_tgt_sld
        })

        df_pred = pd.DataFrame({
            "Thickness": self.all_pred_th,
            "Roughness": self.all_pred_rg,
            "SLD": self.all_pred_sld
        })

        df_error = df_pred - df_target

        fig, axes = plt.subplots(1, 3, figsize=(18, 5))

        sns.heatmap(df_target.corr(), annot=True, cmap="coolwarm", vmin=-1, vmax=1, ax=axes[0])
        axes[0].set_title("Target Correlation")

        sns.heatmap(df_pred.corr(), annot=True, cmap="coolwarm", vmin=-1, vmax=1, ax=axes[1])
        axes[1].set_title("Prediction Correlation")

        sns.heatmap(df_error.corr(), annot=True, cmap="coolwarm", vmin=-1, vmax=1, ax=axes[2])
        axes[2].set_title("Prediction Error Correlation")

        # plt.tight_layout()
        plt.show()



if __name__ == "__main__":
    evaluator = XRREvaluator(
        h5_path=r"D:\03_Resources\Data\XRR_AI\data\p300o6_raw.h5",
        checkpoint_path=r"D:\03_Resources\Data\XRR_AI\model\xrr_model.pt",
        batch_size=64,
        num_workers=0  # Windows 안정성
    )
    metrics = evaluator.evaluate(use_amp=True)
    print(f"Validation Loss: {metrics['loss']:.4f}")
    print(f"Accuracy - Thickness: {metrics['acc_th']:.3f}, Roughness: {metrics['acc_rg']:.3f}, SLD: {metrics['acc_sld']:.3f}")

    evaluator.plot_loss()
    evaluator.plot_error_histogram()
    evaluator.plot_scatter()
    evaluator.plot_correlation()

'''D:\02_Projects\Dev\X-ray_AI\Reflecto\src\reflecto\model.py'''
"""
Improved model utilities:
- Conv1DEncoder and XRRClassifier unchanged in structure but cleaned.
- train_epoch supports AMP via GradScaler, tqdm, gradient clip, and returns parts.
- validate_epoch uses inference_mode and checks for NaN/Inf and clamps logits.
"""

import math

import torch
import torch.nn as nn
from tqdm import tqdm


class Conv1DEncoder(nn.Module):
    def __init__(self, in_channels: int = 1, base_channels: int = 32, n_layers: int = 4, kernel_size: int = 7):
        super().__init__()
        layers = []
        ch = in_channels
        for i in range(n_layers):
            out_ch = base_channels * (2 ** i) if i < 3 else base_channels * (2 ** 3)
            layers.append(nn.Conv1d(ch, out_ch, kernel_size=kernel_size, padding=kernel_size // 2))
            layers.append(nn.BatchNorm1d(out_ch))
            layers.append(nn.ReLU(inplace=True))
            layers.append(nn.MaxPool1d(kernel_size=2, stride=2))
            ch = out_ch
        self.net = nn.Sequential(*layers)
        self.out_channels = ch

    def forward(self, x):
        # input shape: (B, Q) or (B, 1, Q)
        if x.dim() == 2:
            x = x.unsqueeze(1)
        return self.net(x)


class XRRClassifier(nn.Module):
    def __init__(
        self,
        q_len: int,
        n_layers: int,
        n_th_bins: int,
        n_rg_bins: int,
        n_sld_bins: int,
        encoder_channels: int = 32,
        encoder_depth: int = 4,
        layer_hidden: int = 128,
        mlp_hidden: int = 256,
        dropout: float = 0.2,
    ):
        super().__init__()
        self.q_len = q_len
        self.n_layers = n_layers

        self.encoder = Conv1DEncoder(in_channels=1, base_channels=encoder_channels, n_layers=encoder_depth)
        self.global_pool = nn.AdaptiveAvgPool1d(1)

        enc_out_ch = self.encoder.out_channels

        self.bottleneck = nn.Sequential(
            nn.Linear(enc_out_ch, mlp_hidden),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout),
            nn.Linear(mlp_hidden, n_layers * layer_hidden),
        )
        self.layer_hidden = layer_hidden

        # heads
        self.th_head = nn.Sequential(
            nn.Linear(layer_hidden, layer_hidden // 2),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout),
            nn.Linear(layer_hidden // 2, n_th_bins),
        )
        self.rg_head = nn.Sequential(
            nn.Linear(layer_hidden, layer_hidden // 2),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout),
            nn.Linear(layer_hidden // 2, n_rg_bins),
        )
        self.sld_head = nn.Sequential(
            nn.Linear(layer_hidden, layer_hidden // 2),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout),
            nn.Linear(layer_hidden // 2, n_sld_bins),
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.kaiming_uniform_(m.weight, a=math.sqrt(5))
                if m.bias is not None:
                    nn.init.zeros_(m.bias)

    def forward(self, x):
        if x.dim() == 2:
            x = x.unsqueeze(1)
        z = self.encoder(x)
        z = self.global_pool(z)
        z = z.view(z.size(0), -1)

        per_layer = self.bottleneck(z)
        per_layer = per_layer.view(-1, self.n_layers, self.layer_hidden)

        B, L, H = per_layer.shape
        flat = per_layer.view(B * L, H)

        logits_th = self.th_head(flat).view(B, L, -1)
        logits_rg = self.rg_head(flat).view(B, L, -1)
        logits_sld = self.sld_head(flat).view(B, L, -1)

        return logits_th, logits_rg, logits_sld


# -------------------------
# Loss helper
# -------------------------
def multitask_loss(
    logits_th, logits_rg, logits_sld,
    targets,  # (B, L, 3)
    weights=(1.0, 1.0, 1.0),
    ignore_index: int = -100
) -> tuple[torch.Tensor, tuple[float, float, float]]:
    B, L, _ = targets.shape
    device = logits_th.device

    logits_th_f = logits_th.view(B * L, -1)
    logits_rg_f = logits_rg.view(B * L, -1)
    logits_sld_f = logits_sld.view(B * L, -1)

    tgt_th = targets[:, :, 0].reshape(B * L).to(device)
    tgt_rg = targets[:, :, 1].reshape(B * L).to(device)
    tgt_sld = targets[:, :, 2].reshape(B * L).to(device)

    ce = nn.CrossEntropyLoss(ignore_index=ignore_index)
    loss_th = ce(logits_th_f, tgt_th)
    loss_rg = ce(logits_rg_f, tgt_rg)
    loss_sld = ce(logits_sld_f, tgt_sld)

    total = weights[0] * loss_th + weights[1] * loss_rg + weights[2] * loss_sld
    return total, (loss_th.item(), loss_rg.item(), loss_sld.item())


def accuracy_from_logits(logits, targets):
    preds = logits.argmax(dim=-1)
    correct = (preds == targets).float()
    return correct.mean().item()


# -------------------------
# Training / Validation
# -------------------------
def train_epoch(model: nn.Module, loader: torch.utils.data.DataLoader, optimizer: torch.optim.Optimizer,
                device: torch.device, scaler: torch.cuda.amp.GradScaler = None,
                clip_grad: float = 1.0) -> float:
    """
    Single epoch training with optional AMP scaler.
    Returns: (avg_loss, {"th":.., "rg":.., "sld":..})
    """
    model.train()
    ce = nn.CrossEntropyLoss()
    total_loss = 0.0
    last_parts = {"th": 0.0, "rg": 0.0, "sld": 0.0}

    pbar = tqdm(loader, desc="Training", leave=False)
    for refl, labels in pbar:
        refl = refl.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()
        if scaler is not None:
            with torch.amp.autocast('cuda'):
                logits_th, logits_rg, logits_sld = model(refl)
                loss_th = ce(logits_th.view(-1, logits_th.shape[-1]), labels[:, :, 0].view(-1))
                loss_rg = ce(logits_rg.view(-1, logits_rg.shape[-1]), labels[:, :, 1].view(-1))
                loss_sld = ce(logits_sld.view(-1, logits_sld.shape[-1]), labels[:, :, 2].view(-1))
                loss = loss_th + loss_rg + loss_sld
            scaler.scale(loss).backward()
            # optional grad clip
            if clip_grad is not None and clip_grad > 0:
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)
            scaler.step(optimizer)
            scaler.update()
        else:
            logits_th, logits_rg, logits_sld = model(refl)
            loss_th = ce(logits_th.view(-1, logits_th.shape[-1]), labels[:, :, 0].view(-1))
            loss_rg = ce(logits_rg.view(-1, logits_rg.shape[-1]), labels[:, :, 1].view(-1))
            loss_sld = ce(logits_sld.view(-1, logits_sld.shape[-1]), labels[:, :, 2].view(-1))
            loss = loss_th + loss_rg + loss_sld
            loss.backward()
            if clip_grad is not None and clip_grad > 0:
                torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)
            optimizer.step()

        total_loss += loss.item()
        last_parts = {"th": loss_th.item(), "rg": loss_rg.item(), "sld": loss_sld.item()}

        pbar.set_postfix({**last_parts, "loss": f"{loss.item():.4f}"})

    avg = total_loss / len(loader) if len(loader) > 0 else float("nan")
    return avg


def validate_epoch(
    model: nn.Module,
    dataloader: torch.utils.data.DataLoader,
    device: torch.device,
    logits_clip: float = 50.0
) -> tuple[float, dict[str, float]]:

    model.eval()
    total_loss = 0.0
    total_items = 0
    accs = {"th": 0.0, "rg": 0.0, "sld": 0.0}

    with torch.inference_mode():
        pbar = tqdm(dataloader, desc="Validation", leave=False)

        for refl, labels in pbar:
            refl = refl.to(device)
            labels = labels.to(device)

            logits_th, logits_rg, logits_sld = model(refl)

            # --- 안정성 확보: logits clipping ---
            if logits_clip is not None:
                logits_th = logits_th.clamp(-logits_clip, logits_clip)
                logits_rg = logits_rg.clamp(-logits_clip, logits_clip)
                logits_sld = logits_sld.clamp(-logits_clip, logits_clip)

            if not (torch.isfinite(logits_th).all() and 
                    torch.isfinite(logits_rg).all() and 
                    torch.isfinite(logits_sld).all()):
                raise RuntimeError("Non-finite logits encountered in validation.")

            loss, _ = multitask_loss(logits_th, logits_rg, logits_sld, labels)

            bs = refl.size(0)
            total_loss += float(loss.item()) * bs
            total_items += bs

            accs["th"] += accuracy_from_logits(logits_th, labels[..., 0]) * bs
            accs["rg"] += accuracy_from_logits(logits_rg, labels[..., 1]) * bs
            accs["sld"] += accuracy_from_logits(logits_sld, labels[..., 2]) * bs

            pbar.set_postfix({"batch_loss": f"{loss.item():.4f}"})

    avg_loss = total_loss / total_items
    for k in accs:
        accs[k] /= total_items

    return avg_loss, accs

'''D:\02_Projects\Dev\X-ray_AI\Reflecto\src\reflecto\train.py'''
import os

import torch
import torch.nn as nn
from torch.optim import AdamW
from torch.optim.lr_scheduler import StepLR
from torch.utils.data import DataLoader
from tqdm import tqdm

from reflecto.dataset import DatasetH5, ParamQuantizer
from reflecto.model import XRRClassifier, validate_epoch


class XRRTrainer:
    def __init__(
        self,
        h5_path: str,
        batch_size: int = 32,
        epochs: int = 20,
        device: str | torch.device = None,
        lr: float = 1e-3,
        weight_decay: float = 1e-5,
        checkpoint_dir: str = "./checkpoints",
        scheduler_step: int = 5,
        scheduler_gamma: float = 0.5,
    ):
        self.h5_path = h5_path
        self.batch_size = batch_size
        self.epochs = epochs
        self.device = device or torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.lr = lr
        self.weight_decay = weight_decay
        self.checkpoint_dir = checkpoint_dir
        os.makedirs(self.checkpoint_dir, exist_ok=True)

        # Quantizer
        self.quantizer = ParamQuantizer(thickness_bins=None, roughness_bins=None, sld_bins=None)
        self.n_th = len(self.quantizer.thickness_bins) - 1
        self.n_rg = len(self.quantizer.roughness_bins) - 1
        self.n_sld = len(self.quantizer.sld_bins) - 1

        # Dataset
        self.train_dataset = DatasetH5(self.h5_path, self.quantizer, device="cpu")
        self.val_dataset = DatasetH5(self.h5_path, self.quantizer, device="cpu")

        self.train_loader = DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True,
                                       num_workers=4, pin_memory=True)
        self.val_loader = DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False,
                                     num_workers=4, pin_memory=True)

        # Model
        sample_refl, sample_label = self.train_dataset[0]
        self.q_len = sample_refl.shape[0]
        self.n_layers = sample_label.shape[0]
        print(f"Detected q_len: {self.q_len}, n_layers: {self.n_layers}")

        self.model = XRRClassifier(
            q_len=self.q_len,
            n_layers=self.n_layers,
            n_th_bins=self.n_th,
            n_rg_bins=self.n_rg,
            n_sld_bins=self.n_sld
        ).to(self.device)

        self.optimizer = AdamW(self.model.parameters(), lr=self.lr, weight_decay=self.weight_decay)
        self.scheduler = StepLR(self.optimizer, step_size=scheduler_step, gamma=scheduler_gamma)

        self.ce = nn.CrossEntropyLoss()

    def train(self, save_every: int = 5):
        best_val_loss = float("inf")

        for epoch in range(1, self.epochs + 1):
            self.model.train()
            total_loss = 0.0
            last_parts = {"th": 0.0, "rg": 0.0, "sld": 0.0}

            pbar = tqdm(self.train_loader, desc=f"Epoch {epoch}/{self.epochs} [Train]", leave=False)
            for refl, labels in pbar:
                refl = refl.to(self.device)
                labels = labels.to(self.device)

                self.optimizer.zero_grad()
                th_logits, rg_logits, sld_logits = self.model(refl)

                tgt_th = labels[:, :, 0]
                tgt_rg = labels[:, :, 1]
                tgt_sld = labels[:, :, 2]

                loss_th = self.ce(th_logits.view(-1, th_logits.shape[-1]), tgt_th.view(-1))
                loss_rg = self.ce(rg_logits.view(-1, rg_logits.shape[-1]), tgt_rg.view(-1))
                loss_sld = self.ce(sld_logits.view(-1, sld_logits.shape[-1]), tgt_sld.view(-1))

                loss = loss_th + loss_rg + loss_sld
                loss.backward()
                self.optimizer.step()

                total_loss += loss.item()
                last_parts = {"th": loss_th.item(), "rg": loss_rg.item(), "sld": loss_sld.item()}

                # tqdm에 실시간 표시
                pbar.set_postfix({**last_parts, "loss": f"{loss.item():.4f}"})

            avg_train_loss = total_loss / len(self.train_loader)

            # Validation
            val_loss, val_accs = validate_epoch(self.model, self.val_loader, self.device)

            print(
                f"[Epoch {epoch:02d}] "
                f"Train Loss: {avg_train_loss:.4f} | "
                f"Val Loss: {val_loss:.4f} | "
                f"Acc(th/rg/sld): "
                f"{val_accs['th']:.3f} / {val_accs['rg']:.3f} / {val_accs['sld']:.3f}"
            )

            # Scheduler step
            self.scheduler.step()

            # Checkpoints
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                self.save_model(os.path.join(self.checkpoint_dir, "best_model.pt"))
            if epoch % save_every == 0:
                self.save_model(os.path.join(self.checkpoint_dir, f"epoch_{epoch}.pt"))

    def save_model(self, path="xrr_model.pt"):
        torch.save(self.model.state_dict(), path)
        print(f"Model saved to {path}")


if __name__ == "__main__":
    h5_path = r"D:\03_Resources\Data\XRR_AI\data\p300o6_raw.h5"
    trainer = XRRTrainer(h5_path, batch_size=32, epochs=20)
    trainer.train(save_every=5)

'''D:\02_Projects\Dev\X-ray_AI\Reflecto\src\reflecto\__init__.py'''

'''D:\02_Projects\Dev\X-ray_AI\Reflecto\src\reflecto\simulator\noise.py'''
import numpy as np


def apply_poisson_noise(arr: np.ndarray, s: float) -> np.ndarray:
    """Apply Poisson noise to an array."""
    expected_counts = s * arr
    noisy_counts = np.random.poisson(expected_counts)
    return noisy_counts / s

def get_background_noise(count: int, b_min: float, b_max: float) -> np.ndarray:
    """Generate background noise."""
    b = pow(10, np.random.uniform(b_min, b_max))
    return np.random.normal(b, 0.1 * b, count)

def add_noise(R):
    N = len(R)
    R_poisson = apply_poisson_noise(R, s=10 ** 8)
    uniform_noise = 1 + np.random.uniform(-0.1, 0.1, N)
    background_noise = get_background_noise(N, -8, -6)
    curve_scaling = np.random.uniform(0.99, 1.01)
    return R_poisson * uniform_noise * curve_scaling + background_noise
'''D:\02_Projects\Dev\X-ray_AI\Reflecto\src\reflecto\simulator\simulator.py'''
import itertools
from collections.abc import Iterator
from dataclasses import dataclass
from pathlib import Path

import h5py
import numpy as np
from refnx.reflect import SLD, ReflectModel
from refnx.reflect.structure import Stack, Structure
from tqdm import tqdm

from reflecto.simulator.noise import add_noise


@dataclass
class ParamSet:
    thickness: float
    roughness: float
    sld: float

    def to_numpy(self) -> np.ndarray:
        return np.array([self.thickness, self.roughness, self.sld], dtype=np.float64)

    def __format__(self, format_spec: str) -> str:
        """클래스 전체에 포매팅 명령어 적용 (.3f, .2f 등)"""
        formatted = [
            f"{name}={getattr(self, name):{format_spec}}"
            for name in self.__dataclass_fields__.keys()
        ]
        return f"{self.__class__.__name__}({', '.join(formatted)})"


def tth2q_wavelen[T: (float, np.ndarray)](tth: T, wavelen: float = 1.54) -> T:
    """
    Convert 2θ (in degrees) and wavelength (in Å) to scattering vector q (in 1/Å).
    tth: degree
    wavelen: Å
    default Kalpha = 1.54

    -> q: 1/Å
    """
    if 0 > tth > 90:
        raise ValueError(f"tth should have value under (0, 90) not {tth}")
    th_rad = np.radians(tth / 2.0)
    return (4 * np.pi / wavelen) * np.sin(th_rad)


def build_structure(params) -> Structure:
    """refnx Structure를 연속 파라미터로부터 생성."""
    air = SLD(0.0, name="Air")
    substrate = SLD(2.0, name="Substrate")

    stack = Stack(name="MultiLayer", repeats=len(params))

    for param in params:
        mat = SLD(param.sld, name=f"SLD={param.sld}")
        stack.append(mat(param.thickness, param.roughness))

    return air(0, 0) | stack | substrate(0, 3)

def compute_reflectivity(structure: Structure, q: np.ndarray) -> np.ndarray:
    model = ReflectModel(structure)
    return model(q) # nm -> Å

class XRRSimulator:

    def __init__(
            self,
            qs: np.ndarray,
            n_layers:int,
            n_samples: int,
            thickness_range: tuple[float, float] = (5.0, 200.0),
            roughness_range: tuple[float, float] = (0.0, 10.0),
            sld_range: tuple[float, float] = (0.0, 140.0)
            ):
        self.qs = qs
        self.n_layers = n_layers
        self.n_samples = n_samples

        self.thick_range = thickness_range    # nm
        self.rough_range = roughness_range  # Å, 주로 0-10Å 세밀
        self.sld_range = sld_range    # x1e-6 Å^-2

    def make_parameters(self) -> Iterator[tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]]:

        for _ in range(0, self.n_samples):

            thicknesses = np.random.uniform(*self.thick_range, self.n_layers)
            roughnesses = np.random.uniform(*self.rough_range, self.n_layers)
            slds = np.random.uniform(*self.sld_range, self.n_layers)
            params: list[ParamSet] = []
            for t, r, s in zip(thicknesses, roughnesses, slds, strict=True):
                params.append(ParamSet(t, r, s))
                refl = self.simulate_one(params)

            yield thicknesses, roughnesses, slds, refl

    def simulate_one(self, params, has_noise=False) -> np.ndarray:
        structure = build_structure(params)
        refl = compute_reflectivity(structure, self.qs)

        self.refl = add_noise(refl) if has_noise else refl
        return self.refl

    def save_hdf5(self, file: str | Path, show_progress: bool = True) -> None:
        file = Path(file)

        with h5py.File(file, 'w') as hf:
            hf.create_dataset("q", data=self.qs)

            d_thick = hf.create_dataset("thickness", (self.n_samples, self.n_layers), dtype='f4')
            d_rough = hf.create_dataset("roughness", (self.n_samples, self.n_layers), dtype='f4')
            d_sld = hf.create_dataset("sld", (self.n_samples, self.n_layers), dtype='f4')
            d_refl = hf.create_dataset(
                "R", (self.n_samples, len(self.qs)), dtype='f4'
                )

            iterator = self.make_parameters()
            if show_progress:
                iterator = tqdm(
                    iterator,
                    total= self.n_samples,
                    desc="Saving HDF5",
                    dynamic_ncols=True,
                )

            for i, (thicknesses, roughnesses, slds, refl) in enumerate(iterator):

                d_thick[i] = thicknesses
                d_rough[i] = roughnesses
                d_sld[i] = slds
                d_refl[i] = refl



def main() -> None:
    root: Path = Path(r"D:\data\XRR_AI")
    root.mkdir(parents=True, exist_ok=True)
    file: Path = root / "xrr_data.h5"
    # Measurement Configurations
    wavelen: float = 1.54  # (nm)
    tth_min: float = 1.0   # degree
    tth_max: float = 6.0
    q_min: float = tth2q_wavelen(tth_min, wavelen)  # (1/Å)
    q_max: float = tth2q_wavelen(tth_max, wavelen)
    q_n: int = 100
    qs: np.ndarray = np.linspace(q_min, q_max, q_n)

    n_layers: int = 2
    n_samples: int = 100_000
    xrr_simulator: XRRSimulator = XRRSimulator(qs, n_layers, n_samples)
    xrr_simulator.save_hdf5(file)

if __name__ == "__main__":
    main()

'''D:\02_Projects\Dev\X-ray_AI\Reflecto\src\reflecto\simulator\__init__.py'''
